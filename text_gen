#import depz
import numpy
import sys
import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords


from tensorflow.keras.models import Sequential 
from keras.layers import Dense, Dropout, LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint

#load data
#loading data and opening input data in form of text
#proj gutenburg/berg is where data can be found
file = open("frank.txt").read()

#tokenization - breaking a stream of text into words phrases symbols- meaningful elements
#standardization
def tokenize_words(input):
    ##lowercase - to stdz
    input = input.lower()
    #instantiating tokenizer
    tokenizer = RegexpTokenizer(r'\w+')
    #tokenising text to tokens
    tokens = tokenizer.tokenize(input)
    #filtering stopwords
    filtered = filter(lambda token: token not in stopwords.words('english'),tokens)
    return " ".join(filtered)
#preprocess input data, make tokens
processed_inputs = tokenize_words(file)

#chars to nos
#convert char in input to nos
#sort list of set of all char that appear in out i/p text and then use enumerate fn
#to get nos that represent chars
#then create a dict that stores keys and values, or char and nos that represent them
chars = sorted(list(set(processed_inputs)))
char_to_num = dict((c,i) for i, c in enumerate(chars))


#check if words to num/char to num worked
#word to char -work -len
input_len = len(processed_inputs)
vocab_len = len(chars)
print ('total number of char:',input_len)
print('tot vocab:', vocab_len)

#seq len
#def how long we want individual seq here
#individual seq - complete mapping of input char as int
seq_length = 100
x_data = []
y_data = []

#loop thr seq
#v r going thr entire list of i/ps and converting the char to nos with for loop
#this ll create a bunch of seq wher each seq starts with next char is the i/p data beginning with first char
for i in range(0, input_len - seq_length, 1):
    #def i/p -o/p seq
    #i/p - current char + desired seq len
    in_seq = processed_inputs( i + seq_length)
    #o/p - ini char +tot seq
    out_seq = processed_inputs(i + seq_length)
    x_data.append([char_to_num[char] for char in in_seq])
    y_data.append(char_to_num[out_seq])

    #check how many tot i/p seq v ve
n_patterns = len(x_data)
print("tot patterns:", n_patterns)

#convert i/p seq to np array that our network
x = numpy.reshape(x_data, (n_patterns, seq_length, 1))
x = x/float(vocab_len)

#one-hot encoding
y= np_utils.to_categorical(y_data)

#creating model
model = Sequential()
model.add(LSTM(256, input_shape = (x.shape[1], x.shape[2]), return_sequences = True))
model.add(Dropout(0,2))
model.add(LSTM(256,return_sequences = True))
model.add(Dropout(0,2))
model.add(LSTM(128))
model.add(Dropout(0,2))
model.add(Dense(y.shape[1], activation = 'softmax'))

#compile model
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')

#saving wts
filepath = "model_weights_saved.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only=True, mode='min')
desired_callbacks = [checkpoint]

#fit model and let it train
model.fit(x,y,epochs=4,batch_size = 256, callbacks = desired_callbacks)

#recompile model with saved wts
filename = "model_weights_saved.hdf5"
model.load_weights(filename)
model.compile(loss = "categorical_crossentropy', optimizer = 'adam')

#o/p of model back into chars
num_to_char = dict(i,c) for i,c in enumerate(chars)

#random seed to help gen
start = numpy.random.randint(0, len(x_data) - 1)
pattern = x_data[start]
print("random seed:")
print("\"",''.join([num_to_char[value] for value in pattern]), "\"")

#gen text
for i in range(1000):
    x = numpy.reshape(pattern,(1,len(pattern),1))
    x = x/float(vocab_len)
    prediction = model.predict(x, verbose = 0)
    index = numpy.argmax(prediction)
    result = num_to_char[index]
    seq_in = [num_to_char[value] for value in pattern]
    sys.stdout.write(result)
    pattern.append(index)
    pattern = pattern [1:len(pattern)]



